name: Periodic Book Scraper

on:
  schedule:
    - cron: '0 0 */7 * *'  # Runs every 7 days
  workflow_dispatch:  # Allows manual triggering
  push:
    paths:
      - 'config.json'  # Run when config is updated

# Add these permissions
permissions:
  contents: write
  
jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create necessary directories
      run: |
        mkdir -p downloads
    
    - name: Setup Google Drive credentials
      run: |
        echo '${{ secrets.GOOGLE_CREDENTIALS }}' > credentials.json
    
    - name: Initialize processed publications
      run: |
        mkdir -p data
        echo '{"processed_publications":[]}' > data/processed_publications.json
    
    - name: Run scraper
      env:
        EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      run: |
        python scripts/scraper_worker.py
    
    - name: Commit processed publications
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add data/processed_publications.json
        git commit -m "Update processed publications log" || echo "No changes to commit"
        git push